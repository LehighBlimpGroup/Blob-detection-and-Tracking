# Blob-detection-and-Tracking
The source code for the perception module of individual blimps in e MochiSwarm. It has two modes that is manually selectable by changing the [`mode` on line 1915](https://github.com/LehighBlimpGroup/Blob-detection-and-Tracking/blob/main/perception_subsystem.py#L1915): `mode = 0` for balloon detection based on LAB color family detection, and `mode = 1` for goal detection based on blob detection. Currently, the program supports Arduino NiclaVision and OpenMV RT1062. The color family data and the thresholds are tuned for the NiclaVision board. 

## Installation 
The program `perception_subsystem.py` needs to be loaded to a NiclaVision or an opened camera using [OpenMV IDE](https://openmv.io/pages/download?srsltid=AfmBOoqBGVMSZ4dXU9Bmfq7aGBJ4bq8ShArE3y7I8h9C1_Ys06h6fQ3-). Simply open `perception_subsystem.py` in the OpenMV IDE, and click the green play button to run the program on the camera board. Saving the program to flash need an additional step that is [specified in the OpenMV IDE documentation](https://docs.openmv.io/openmvcam/tutorial/openmvide_overview.html#tools). Make sure the board is running an OpenMV firmware especially for the NiclaVision. If not, when connecting the camera to the OpenMV IDE for the first time, the IDE will prompt to flash the board with the required firmware.

## Expected behavior after first installation 
After loading the `perceptIon_subsystem.py` to NiclaVision or OpenMV RT1062, you may see green/pink cells blinking in the image buffer (or nothing at all on top of the camera image, depending on how much the `COLOR_OATA` on [line 79-92](https://github.com/LehighBlimpGroup/Blob-detection-and-Tracking/blob/main/perception_subsystem.py#L79-L92) is deviated from the actual desired color under the current lighting conditions and the individual differences of the image sensor.) Setting `mode = l` will give less volatile results as the goal detection enables auto white balance. 

## Tuning colors
1. For color family detection (`mode = 0`), follow [the steps in the color training repository](https://github.com/LehighBlimpGroup/balltraining?tab=readme-ov-file#steps-for-obtaining-lab-color-space-information), and copy the color data from the terminal to the specific color of calibration to the item in `COLOR_OATA`. You need a separate python environment that is not on the OpenMV IDE to run [the training script](https://github.com/LehighBlimpGroup/balltraining/blob/main/gaussian_manual_multi.py). Enable detection of the calibrated colors by adding entries to the `COLOR_TARGET` dictionary on [line 117-118](https://github.com/LehighBlimpGroup/Blob-detection-and-Tracking/blob/main/perception_subsystem.py#L117C1-L118).
2. For blob detection (`mode = 1`), check [the OpenMV documentation on the find_blobs function](https://docs.openmv.io/library/omv.image.html#image.Image.find_blobs) for calibrating the colors. You need to change the 6-element array on [line 178-180](https://github.com/LehighBlimpGroup/Blob-detection-and-Tracking/blob/main/perception_subsystem.py#L178-L180) for your desired color to track, and specify the target on [line 181](https://github.com/LehighBlimpGroup/Blob-detection-and-Tracking/blob/main/perception_subsystem.py#L181).

## To be merged with changes made during DTR Fall 2024
1. 4 modes instead of 2, accounting for different target colors.
2. For orange goals, the blob center is tested using the LAB color family detection to eliminate the interference from the red background and other red balloons. 
